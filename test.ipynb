{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71494,"status":"ok","timestamp":1730235746416,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"},"user_tz":-60},"id":"CpHZ-OUjkP9a","outputId":"bbc884fd-7833-49a9-fde7-5267d348f357"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.7/877.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n","!pip install -q git+https://github.com/huggingface/transformers.git\n","!pip install -q datasets\n","!pip install -q monai\n","!pip install -q ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5639,"status":"ok","timestamp":1730235810777,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"},"user_tz":-60},"id":"D42LpxQFj4qE","outputId":"6b88db86-23ee-4beb-c768-6120cd6f581f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import random\n","from scipy import ndimage\n","from DataPreprocess import ImageData, MaskData, BBox, Reshape\n","from ultralytics import YOLO\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dA8xg3Zw1eIG"},"outputs":[],"source":["from helper import Helper\n","\n","helperfns = Helper()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19216,"status":"ok","timestamp":1730235835134,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"},"user_tz":-60},"id":"R4Oa6XnfkC8a","outputId":"787d5e34-5c01-4f35-9b89-e55e31829e9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mmvVIXJkE8s"},"outputs":[],"source":["ROOT_DIR = '/content/gdrive/My Drive/coding/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0wqSevrkRGD"},"outputs":[],"source":["folder_path = ROOT_DIR + 'dl_challenge'\n","\n","image_data = ImageData()\n","mask_data = MaskData()\n","resize_data = Reshape(256, 256)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wQZSbygkVss"},"outputs":[],"source":["image_data.read_all_files(folder_path)\n","mask_data.read_all_files(folder_path)\n","combined_masks = resize_data.combine_masks(mask_data.masks)\n","for i in range(len(combined_masks)):\n","    combined_masks[i] = combined_masks[i].astype(np.uint8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wgb38fLJkjH_"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","images_train, images_temp, combined_masks_train, combined_masks_temp, mask_data_train, mask_data_temp = train_test_split(\n","    image_data.images, combined_masks, mask_data.masks, test_size=0.2, random_state=42\n",")\n","\n","images_val, images_test, combined_masks_val, combined_masks_test, mask_data_val, mask_data_test = train_test_split(\n","    images_temp, combined_masks_temp, mask_data_temp, test_size=0.5, random_state=42\n",")"]},{"cell_type":"markdown","metadata":{"id":"K4-GJLcmDeSR"},"source":["# Yolo Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iqw3rE86DcSl"},"outputs":[],"source":["model_path = ROOT_DIR + \"yolov8_instance_seg/models/best.pt\"\n","\n","model = YOLO(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12694,"status":"ok","timestamp":1730138784927,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"},"user_tz":-60},"id":"gLvks6TgG7W_","outputId":"e6c6c2a4-aff8-46e7-bb93-00df971b53de"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","0: 576x640 (no detections), 582.8ms\n","Speed: 30.6ms preprocess, 582.8ms inference, 17.0ms postprocess per image at shape (1, 3, 576, 640)\n","\n","0: 480x640 2 objects, 266.2ms\n","Speed: 5.3ms preprocess, 266.2ms inference, 31.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 448x640 6 objects, 237.7ms\n","Speed: 6.9ms preprocess, 237.7ms inference, 24.2ms postprocess per image at shape (1, 3, 448, 640)\n","\n","0: 512x640 9 objects, 283.7ms\n","Speed: 4.5ms preprocess, 283.7ms inference, 32.3ms postprocess per image at shape (1, 3, 512, 640)\n","\n","0: 544x640 3 objects, 308.6ms\n","Speed: 5.1ms preprocess, 308.6ms inference, 21.4ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 544x640 5 objects, 257.5ms\n","Speed: 5.4ms preprocess, 257.5ms inference, 16.1ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 544x640 4 objects, 235.8ms\n","Speed: 4.7ms preprocess, 235.8ms inference, 14.1ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 416x640 3 objects, 221.1ms\n","Speed: 6.1ms preprocess, 221.1ms inference, 12.8ms postprocess per image at shape (1, 3, 416, 640)\n","\n","0: 576x640 2 objects, 264.2ms\n","Speed: 4.9ms preprocess, 264.2ms inference, 8.4ms postprocess per image at shape (1, 3, 576, 640)\n","\n","0: 544x640 1 object, 254.1ms\n","Speed: 4.6ms preprocess, 254.1ms inference, 6.5ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 544x640 5 objects, 244.6ms\n","Speed: 8.6ms preprocess, 244.6ms inference, 16.2ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 512x640 (no detections), 245.7ms\n","Speed: 5.9ms preprocess, 245.7ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n","\n","0: 480x640 4 objects, 224.0ms\n","Speed: 5.6ms preprocess, 224.0ms inference, 13.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 576x640 1 object, 257.9ms\n","Speed: 5.8ms preprocess, 257.9ms inference, 4.8ms postprocess per image at shape (1, 3, 576, 640)\n","\n","0: 480x640 6 objects, 212.2ms\n","Speed: 4.4ms preprocess, 212.2ms inference, 17.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 544x640 4 objects, 269.5ms\n","Speed: 4.2ms preprocess, 269.5ms inference, 15.2ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 544x640 9 objects, 246.6ms\n","Speed: 5.0ms preprocess, 246.6ms inference, 28.3ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 480x640 6 objects, 215.3ms\n","Speed: 4.5ms preprocess, 215.3ms inference, 17.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 640x640 1 object, 315.6ms\n","Speed: 6.3ms preprocess, 315.6ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 576x640 6 objects, 268.7ms\n","Speed: 3.6ms preprocess, 268.7ms inference, 21.6ms postprocess per image at shape (1, 3, 576, 640)\n"]}],"source":["iou_score_list = []\n","gt_obj_list = []\n","detected_obj_list = []\n","for image, gt_mask, obj_mask in zip(images_test, combined_masks_test, mask_data_test):\n","    masks_present = False\n","    results = model(image)\n","    for result in results:\n","        if result.masks:\n","            masks_present = True\n","            detected_obj_list.append(result.masks.data.shape[0])\n","\n","    if masks_present:\n","        yolo_seg_mask = helperfns.generate_mask(image, results)\n","        yolo_combined_mask = helperfns.combine_masks(yolo_seg_mask)\n","        iou_score = helperfns.calculate_iou(yolo_combined_mask, gt_mask)\n","        iou_score_list.append(iou_score)\n","        gt_obj_list.append(obj_mask.shape[0])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1730138791075,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"},"user_tz":-60},"id":"vz65jPVaHvCO","outputId":"99589540-ea2e-4ec9-851b-856bcc7b7397"},"outputs":[{"name":"stdout","output_type":"stream","text":["mean IOU: 0.7138095065090015\n"]}],"source":["if len(iou_score_list) > 0:\n","    mean_value = sum(iou_score_list) / len(iou_score_list)\n","else:\n","    mean_value = 0\n","print(f'Mean IoU: {mean_value}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":315,"status":"ok","timestamp":1730139268069,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"},"user_tz":-60},"id":"KT6JoOwcUT_T","outputId":"c760d26a-9276-4f07-d9c0-60f621357605"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Absolute Error: 3.888888888888889\n"]}],"source":["mean_abs_error = np.mean(np.abs(np.array(gt_obj_list) - np.array(detected_obj_list)))\n","print(f'Mean Absolute Error: {mean_abs_error}')"]},{"cell_type":"markdown","metadata":{"id":"fz_IrugkQIX6"},"source":["# SAM Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EASx54PQJ2U"},"outputs":[],"source":["from ultralytics import YOLO\n","from segment_anything import sam_model_registry, SamPredictor\n","model_path = ROOT_DIR + 'yolov8/models/best.pt'\n","model = YOLO(model_path)\n","\n","\n","sam_checkpoint = ROOT_DIR + \"SAM/models/best.pth\"\n","model_type = \"vit_b\"\n","\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","predictor = SamPredictor(sam)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9140,"status":"ok","timestamp":1730236175424,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"},"user_tz":-60},"id":"umNI91kfi64K","outputId":"4d78eb90-32a3-4e9b-cfcd-1a2d720511d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 576x640 2 objects, 9.7ms\n","Speed: 5.0ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 640)\n","\n","0: 480x640 2 objects, 7.9ms\n","Speed: 3.0ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 448x640 12 objects, 7.6ms\n","Speed: 2.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n","\n","0: 512x640 14 objects, 7.1ms\n","Speed: 4.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n","\n","0: 544x640 3 objects, 7.4ms\n","Speed: 3.2ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 544x640 17 objects, 11.3ms\n","Speed: 4.5ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 544x640 21 objects, 10.5ms\n","Speed: 3.3ms preprocess, 10.5ms inference, 1.8ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 416x640 6 objects, 10.3ms\n","Speed: 2.5ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n","\n","0: 576x640 2 objects, 12.0ms\n","Speed: 3.7ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n","\n","0: 544x640 5 objects, 10.2ms\n","Speed: 3.5ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 544x640 12 objects, 11.5ms\n","Speed: 3.3ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 512x640 4 objects, 12.3ms\n","Speed: 3.4ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 512, 640)\n","\n","0: 480x640 12 objects, 13.4ms\n","Speed: 3.8ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 576x640 2 objects, 12.8ms\n","Speed: 3.8ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 576, 640)\n","\n","0: 480x640 16 objects, 7.4ms\n","Speed: 4.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 544x640 4 objects, 7.7ms\n","Speed: 3.2ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 544x640 10 objects, 6.4ms\n","Speed: 3.0ms preprocess, 6.4ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n","\n","0: 480x640 5 objects, 7.3ms\n","Speed: 3.4ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 640x640 1 object, 8.3ms\n","Speed: 3.8ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 576x640 14 objects, 7.5ms\n","Speed: 4.3ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n"]}],"source":["iou_score_list = []\n","gt_obj_list = []\n","detected_obj_list = []\n","for image, gt_mask, obj_mask in zip(images_test, combined_masks_test, mask_data_test):\n","    results = model(image)\n","    yolo_boxes = results[0].boxes\n","    detected_obj_list.append(len(yolo_boxes))\n","    yolo_boxes_list = yolo_boxes.xyxy.cpu().numpy().tolist()\n","    predictor.set_image(image)\n","    input_boxes = torch.tensor(yolo_boxes_list, device=predictor.device)\n","    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n","    masks, _, _ = predictor.predict_torch(\n","        point_coords=None,\n","        point_labels=None,\n","        boxes=transformed_boxes,\n","        multimask_output=False,\n","    )\n","    sam_seg_mask = masks.squeeze(1).cpu().numpy()\n","    sam_combined_mask = helperfns.combine_masks(sam_seg_mask)\n","    iou_score = helperfns.calculate_iou(sam_combined_mask, gt_mask)\n","    iou_score_list.append(iou_score)\n","    gt_obj_list.append(obj_mask.shape[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1730236178504,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"},"user_tz":-60},"id":"2cUMjVeM0qkJ","outputId":"a403e3a8-e289-4b6b-a7da-9851bd760244"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean IoU: 0.8047080937797743\n"]}],"source":["if len(iou_score_list) > 0:\n","    mean_value = sum(iou_score_list) / len(iou_score_list)\n","else:\n","    mean_value = 0\n","print(f'Mean IoU: {mean_value}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXyNrS_M3Fxx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730236180319,"user_tz":-60,"elapsed":301,"user":{"displayName":"Prathvish Mithare","userId":"09096248188935264467"}},"outputId":"e9a4de64-778a-4e16-b98d-b12bb00b3b89"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error: 1.2\n"]}],"source":["mean_abs_error = np.mean(np.abs(np.array(gt_obj_list) - np.array(detected_obj_list)))\n","print(f'Mean Absolute Error: {mean_abs_error}')"]},{"cell_type":"code","source":[],"metadata":{"id":"5D0GyQuWG25Z"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}